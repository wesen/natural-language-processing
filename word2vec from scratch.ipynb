{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The softmax function\n",
    "\n",
    "The softmax function allows us to transform a vector of real numbers into a probability distribution. Its formula is:\n",
    "\n",
    "$$\n",
    "\\mathrm{softmax}(x, i) = \\frac{e^x_i}{\\sum_{j=1}^{n}{e^x_j}}\n",
    "$$\n",
    "\n",
    "$e^x$ gets huge quickly, and makes us run into numerical limitation. We can show that \n",
    "$$\n",
    "\\mathrm{softmax}(x,i) = \\frac{e^{x_i-c}}{\\sum_{j=1}^{n}{e^{x_j - c}}}\n",
    "$$\n",
    "which allows us to subtract the maximum value of our vector from each element to avoid numerical issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    orig_shape = x.shape\n",
    "    \n",
    "    if len(x.shape) > 1:\n",
    "        # Matrix\n",
    "        c = -np.array([np.max(x, axis=1)]).T\n",
    "        e_x = np.exp(x + c)\n",
    "        _sum = e_x.sum(axis=1)\n",
    "        x = e_x / _sum[:, None]\n",
    "    else:\n",
    "        # Vector\n",
    "        c = -np.max(x)\n",
    "        e_x = np.exp(x + c)\n",
    "        _sum = np.sum(e_x)\n",
    "        x = e_x / _sum\n",
    "        \n",
    "    assert x.shape == orig_shape\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When implementing derivative functions, it's quite useful to check intermediate results before going wildly off tangent. An easy way to do this is to compute the numerical gradient, and compare the two values.\n",
    "\n",
    "We use a very simple numerical calculation of the gradient:\n",
    "\n",
    "$$\n",
    "\\mathrm{grad}(f(x)) = \\frac{f(x+h) - f(x-h)}{2h}\n",
    "$$\n",
    "\n",
    "We use [numpy multi-index iteration](https://docs.scipy.org/doc/numpy-1.14.0/reference/arrays.nditer.html#tracking-an-index-or-multi-index)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive gradient checking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_gradient(f, x):\n",
    "    rnd_state = random.getstate()\n",
    "    random.setstate(rnd_state)\n",
    "\n",
    "    fx, grad = f(x)\n",
    "    h = 1e-4\n",
    "    \n",
    "    # x can be a vector or a matrix, so we want to compute the derivative according to each element.\n",
    "    # This is straight out of the cs224n code.\n",
    "    it = np.nditer(x, flags=['multi_index'], op_flags=['readwrite'])\n",
    "    while not it.finished:\n",
    "        ix = it.multi_index\n",
    "        \n",
    "        x[ix] += h\n",
    "        random.setstate(rnd_state)\n",
    "\n",
    "        fx_1, _ = f(x)\n",
    "        random.setstate(rnd_state)\n",
    "\n",
    "        x[ix] -= 2 * h\n",
    "        fx_2, _ = f(x)\n",
    "        x[ix] += h\n",
    "        \n",
    "        numgrad = (fx_1 - fx_2) / (2 * h)\n",
    "        \n",
    "        # we can now check the numerical gradient against the computed gradient.\n",
    "        reldiff = abs(numgrad - grad[ix]) / max(1, abs(numgrad), abs(grad[ix]))\n",
    "        if reldiff > 1e-5:\n",
    "            print(\"Gradient check failed at index {}: {} should be {}\".format(ix, grad[ix], numgrad))\n",
    "            return\n",
    "        \n",
    "        it.iternext()\n",
    "    \n",
    "    print(\"Gradients seem to be ok!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check out the gradient checking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradients seem to be ok!\n"
     ]
    }
   ],
   "source": [
    "def test_gradient_f(x):\n",
    "    return 2. * x[0], np.ones_like(x) * 2.\n",
    "\n",
    "check_gradient(test_gradient_f, np.array([1.]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Row normalization\n",
    "\n",
    "It is useful to normalize the rows of our word2vec embeddings on each training iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_rows(x):\n",
    "    return x / np.sqrt(np.sum(x * x, axis=1))[:, None]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SGD\n",
    "\n",
    "Here we implement stochastic gradient descent. We iterate over our data in batches of pairs (input, target). We compute our own result (forward-pass), take the gradient wrt the target, and propagate the gradient backwards, updating our variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sgd(f, x0, step, iterations, postprocessing=None, PRINT_EVERY=100):\n",
    "    start_iter = 0\n",
    "    \n",
    "    ANNEAL_EVERY = 20000\n",
    "    \n",
    "    if postprocessing is None:\n",
    "        postprocessing = lambda x: x\n",
    "        \n",
    "    x = x0\n",
    "    \n",
    "    for i in range(start_iter + 1, iterations + 1):\n",
    "        cost, gradient = f(x)\n",
    "        x -= step * gradient\n",
    "        postprocessing(x)\n",
    "        \n",
    "        if i % PRINT_EVERY == 0:\n",
    "            print(\"iteration {}: {:.4f}\".format(i, cost))\n",
    "\n",
    "        if i % ANNEAL_EVERY == 0:\n",
    "            step *= 0.5\n",
    "            \n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test the gradient descent on a very simple function:\n",
    "\n",
    "$$\n",
    "\\sum_{i=1}^{n}{x_i}^2\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quad(x):\n",
    "    return np.sum(x ** 2), x * 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 100: 0.0046\n",
      "iteration 200: 0.0001\n",
      "iteration 300: 0.0000\n",
      "iteration 400: 0.0000\n",
      "iteration 500: 0.0000\n",
      "iteration 600: 0.0000\n",
      "iteration 700: 0.0000\n",
      "iteration 800: 0.0000\n",
      "iteration 900: 0.0000\n",
      "iteration 1000: 0.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "8.414836786079764e-10"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sgd(f=quad, x0=0.5, step=0.01, iterations=1000, PRINT_EVERY=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 100: 0.0000\n",
      "iteration 200: 0.0000\n",
      "iteration 300: 0.0000\n",
      "iteration 400: 0.0000\n",
      "iteration 500: 0.0000\n",
      "iteration 600: 0.0000\n",
      "iteration 700: 0.0000\n",
      "iteration 800: 0.0000\n",
      "iteration 900: 0.0000\n",
      "iteration 1000: 0.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sgd(f=quad, x0=0.0, step=0.01, iterations=1000, PRINT_EVERY=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 100: 1.5137\n",
      "iteration 200: 1.0142\n",
      "iteration 300: 0.6796\n",
      "iteration 400: 0.4554\n",
      "iteration 500: 0.3051\n",
      "iteration 600: 0.2044\n",
      "iteration 700: 0.1370\n",
      "iteration 800: 0.0918\n",
      "iteration 900: 0.0615\n",
      "iteration 1000: 0.0412\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "-0.2025967836700259"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sgd(f=quad, x0=-1.5, step=0.001, iterations=1000, PRINT_EVERY=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Skip gram\n",
    "\n",
    "We implement the skip-gram model. In skip-gram, we are going to predict the context words for every center word.\n",
    "Skip-gram uses softmax with log-entropy as a loss function. We derive and implement the necessary functions (derivative according to the predicted vector, and derivative according to all the output vectors)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax_cost_and_gradient(predicted, target, output_vectors):\n",
    "    y_hat = softmax(np.dot(predicted, output_vectors.T))\n",
    "    cost = -np.log(y_hat[target])\n",
    "    \n",
    "    grad = np.outer(y_hat, predicted)\n",
    "    grad[target] -= predicted\n",
    "    grad_predicted = -output_vectors[target] + np.sum(y_hat[:, None] * output_vectors, axis=0)\n",
    "    \n",
    "    return cost, grad_predicted, grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test the softmax_cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradients seem to be ok!\n",
      "Gradients seem to be ok!\n"
     ]
    }
   ],
   "source": [
    "Ninner = 3\n",
    "Nwords = 5\n",
    "vc = np.random.rand(Ninner)\n",
    "target = 1\n",
    "output_vectors = np.random.rand(Ninner * Nwords).reshape((Nwords, Ninner))\n",
    "\n",
    "def softmax_cost_and_gradient_predicted(vc, output_vectors):\n",
    "    cost, grad_predicted, grad = softmax_cost_and_gradient(vc, target, output_vectors)\n",
    "    return cost, grad_predicted\n",
    "\n",
    "def softmax_cost_and_gradient_grad(vc, output_vectors):\n",
    "    cost, grad_predicted, grad = softmax_cost_and_gradient(vc, target, output_vectors)\n",
    "    return cost, grad\n",
    "\n",
    "check_gradient(lambda output_vectors: softmax_cost_and_gradient_grad(vc, output_vectors), output_vectors)\n",
    "check_gradient(lambda vc: softmax_cost_and_gradient_predicted(vc, output_vectors), vc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "def skip_gram(current_word, context_size, context_words, tokens, input_vectors, output_vectors):\n",
    "    cost = 0.\n",
    "    grad_in = np.zeros(input_vectors.shape)\n",
    "    grad_out = np.zeros(output_vectors.shape)\n",
    "    \n",
    "    center_index = tokens[current_word]\n",
    "    \n",
    "#     print(\"current_word: {}, context_words: {}\".format(current_word, context_words))\n",
    "    \n",
    "    for word in context_words:\n",
    "        target = tokens[word]\n",
    "        cost_, grad_predicted_, grad_ = softmax_cost_and_gradient(predicted=input_vectors[center_index],\n",
    "                                                                 target=target,\n",
    "                                                                 output_vectors=output_vectors)\n",
    "        cost += cost_\n",
    "        grad_in[center_index] += grad_predicted_\n",
    "        grad_out += grad_\n",
    "        \n",
    "    return cost, grad_in, grad_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now plug this into our sgd."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word2vec_sgd(tokens, word_vectors, context_size, dataset):\n",
    "    batch_size = 50\n",
    "    cost = 0.0\n",
    "    grad = np.zeros(word_vectors.shape)\n",
    "\n",
    "    N = word_vectors.shape[0]\n",
    "    \n",
    "    # our word vectors are going to be split in 2 for the purpose of training\n",
    "    input_vectors = word_vectors[:int(N / 2), :]\n",
    "    output_vectors = word_vectors[int(N / 2):, :]\n",
    "    \n",
    "    for i in range(batch_size):\n",
    "        context_size_ = random.randint(1, context_size)\n",
    "        center_word, context_words = dataset.getRandomContext(context_size_)\n",
    "        \n",
    "        cost_, grad_in_, grad_out_ = skip_gram(center_word, \n",
    "                                               context_size=context_size, \n",
    "                                               context_words=context_words,\n",
    "                                               tokens=tokens, \n",
    "                                               input_vectors=input_vectors, \n",
    "                                               output_vectors=output_vectors)\n",
    "        \n",
    "        cost += cost_ / batch_size\n",
    "        grad[:int(N / 2), :] += grad_in_ / batch_size\n",
    "        grad[int(N / 2):, :] += grad_out_ / batch_size\n",
    "    \n",
    "#     print(\"cost: {}, grad: {}\".format(cost, grad))\n",
    "    return cost, grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do some tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = type('dummy', (), {})()\n",
    "\n",
    "def dummySampleTokenIdx():\n",
    "    return random.randint(0, 4)\n",
    "\n",
    "def getRandomContext(C):\n",
    "    tokens = [\"a\", \"b\", \"c\", \"d\", \"e\"]\n",
    "    return tokens[random.randint(0, 4)], \\\n",
    "           [tokens[random.randint(0, 4)] for i in range(2 * C)]\n",
    "\n",
    "dataset.sampleTokenIdx = dummySampleTokenIdx\n",
    "dataset.getRandomContext = getRandomContext\n",
    "\n",
    "random.seed(31415)\n",
    "np.random.seed(9265)\n",
    "dummy_vectors = normalize_rows(np.random.randn(10, 3))\n",
    "dummy_tokens = dict([(\"a\", 0), (\"b\", 1), (\"c\", 2), (\"d\", 3), (\"e\", 4)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradients seem to be ok!\n"
     ]
    }
   ],
   "source": [
    "check_gradient(lambda vec: word2vec_sgd(dummy_tokens, vec, context_size=5, dataset=dataset), dummy_vectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input dataset\n",
    "\n",
    "We can now take a look at the input data we are going to be using.    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.treebank import StanfordSentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = StanfordSentiment()\n",
    "tokens = dataset.tokens()\n",
    "n_words = len(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset is a big dict of words to their index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[b'six-packs',\n",
       " b'banking',\n",
       " b'contributed',\n",
       " b'sex-as-war',\n",
       " b'anakin',\n",
       " b'career-defining',\n",
       " b'tension',\n",
       " b'muccino',\n",
       " b'sequence',\n",
       " b'nightmares']"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(tokens.keys())[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7233"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens[b'banking']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 10: 27.0717\n",
      "iteration 20: 28.0597\n",
      "iteration 30: 26.4789\n",
      "iteration 40: 27.2693\n",
      "iteration 50: 31.0237\n",
      "iteration 60: 29.2453\n",
      "iteration 70: 26.4788\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-179-d08e264673f0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0mstep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0miterations\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m40000\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m     PRINT_EVERY=10)\n\u001b[0m",
      "\u001b[0;32m<ipython-input-152-b8fc00ad8272>\u001b[0m in \u001b[0;36msgd\u001b[0;34m(f, x0, step, iterations, postprocessing, PRINT_EVERY)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstart_iter\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterations\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m         \u001b[0mcost\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mpostprocessing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-179-d08e264673f0>\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(word_vectors)\u001b[0m\n\u001b[1;32m     16\u001b[0m                                         \u001b[0mword_vectors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mword_vectors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m                                         \u001b[0mcontext_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcontext_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m                                         dataset=dataset),\n\u001b[0m\u001b[1;32m     19\u001b[0m     \u001b[0mx0\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mword_vectors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0mstep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-169-175d63964bd3>\u001b[0m in \u001b[0;36mword2vec_sgd\u001b[0;34m(tokens, word_vectors, context_size, dataset)\u001b[0m\n\u001b[1;32m     19\u001b[0m                                                \u001b[0mtokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m                                                \u001b[0minput_vectors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_vectors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m                                                output_vectors=output_vectors)\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0mcost\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mcost_\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-165-a3c291063b37>\u001b[0m in \u001b[0;36mskip_gram\u001b[0;34m(current_word, context_size, context_words, tokens, input_vectors, output_vectors)\u001b[0m\n\u001b[1;32m     12\u001b[0m         cost_, grad_predicted_, grad_ = softmax_cost_and_gradient(predicted=input_vectors[center_index],\n\u001b[1;32m     13\u001b[0m                                                                  \u001b[0mtarget\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m                                                                  output_vectors=output_vectors)\n\u001b[0m\u001b[1;32m     15\u001b[0m         \u001b[0mcost\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mcost_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mgrad_in\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcenter_index\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mgrad_predicted_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-157-d6c21ca9f631>\u001b[0m in \u001b[0;36msoftmax_cost_and_gradient\u001b[0;34m(predicted, target, output_vectors)\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mgrad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mouter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_hat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredicted\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mgrad\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0mpredicted\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mgrad_predicted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0moutput_vectors\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_hat\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0moutput_vectors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mcost\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_predicted\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "dataset = StanfordSentiment()\n",
    "tokens = dataset.tokens()\n",
    "n_words = len(tokens)\n",
    "\n",
    "dim_vectors = 10\n",
    "context_size = 5\n",
    "\n",
    "random.seed(31415)\n",
    "np.random.seed(9265)\n",
    "\n",
    "word_vectors = np.concatenate(\n",
    "    ((np.random.rand(n_words, dim_vectors) - 0.5) / dim_vectors, np.zeros((n_words, dim_vectors))), \n",
    "    axis=0)\n",
    "word_vectors = sgd(\n",
    "    f=lambda word_vectors: word2vec_sgd(tokens=tokens, \n",
    "                                        word_vectors=word_vectors, \n",
    "                                        context_size=context_size, \n",
    "                                        dataset=dataset),\n",
    "    x0=word_vectors,\n",
    "    step=0.3,\n",
    "    iterations=40000,\n",
    "    PRINT_EVERY=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
